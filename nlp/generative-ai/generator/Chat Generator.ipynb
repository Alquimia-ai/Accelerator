{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5176f39-202d-4954-ac70-aeb6eef4b677",
   "metadata": {},
   "source": [
    "# <center> üè≠ Chat Generator üó£Ô∏èüí¨\n",
    "\n",
    "The Chat Generator Notebook is a tool designed to **generate synthetic conversational datasets using a LLM**. It supports custom project configurations, dynamic prompt generation, and conversation simulation while leveraging project-specific knowledge bases and templates. \n",
    "    \n",
    "The notebook allows for scalable conversation generation and labeling, suitable for **training or testing ML models**.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347f3817-4d2b-4d27-b3d6-244fb3a8ed12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4684f1a7-609c-4659-8685-ea7ad68f1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.my_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e92c11c-6971-4a30-9c50-a1fa2253d699",
   "metadata": {},
   "source": [
    "## Environment Variables üåé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63aaf77-ed9a-46f6-b0b7-e8b466f4de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set up API parameters to call the model\n",
    "LLM_API_URL = os.environ.get(\"LLM_API_URL\")\n",
    "LLM_API_KEY = os.environ.get(\"LLM_API_KEY\")\n",
    "LLM_NAME = os.environ.get(\"LLM_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee195f-1e14-48b6-ba72-6f3782dce043",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Input Variables ‚úçÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87387254-631b-4426-812a-ab7e5952ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, enter the following information...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\t- project name: \n"
     ]
    }
   ],
   "source": [
    "### Enter input-variables...\n",
    "print(\"Please, enter the following information...\\n\")\n",
    "\n",
    "# project name\n",
    "project_name = input(\"\\t- project name:\")\n",
    "\n",
    "# number of iterations\n",
    "iterations = int(input(\"\\t- iterations:\"))\n",
    "\n",
    "# number of exchanges in conversation\n",
    "k = int(input(\"\\t- max number of exchanges per generated conversation:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced7405-f174-49f8-a06c-3fc5b5cbf92d",
   "metadata": {},
   "source": [
    "## Read `.json` config Files üìùüìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09466f-d30e-477d-9061-ac0f659c609b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to read .json files\n",
    "def read_json_files(directory_path):\n",
    "    json_data = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                value = json.load(file)\n",
    "                key = filename.split(\".json\")[0]\n",
    "                json_data[key] = value\n",
    "    return json_data\n",
    "\n",
    "# Read config files\n",
    "config_path = f\"projects/{project_name}/config\"\n",
    "configs = read_json_files(config_path)\n",
    "\n",
    "# Read prompt files\n",
    "injections_path = f\"projects/{project_name}/prompting/injections\"\n",
    "injections = read_json_files(injections_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d85985-d816-46f7-ac60-4230b12656c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read `.txt` prompt and example Files üìùüìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64468d-eff9-4afe-b8b6-e89e6d0b795e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read .txt files\n",
    "def read_txt_files(directory_path):\n",
    "    text_data = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                value = file.read()\n",
    "                key = filename.split(\".txt\")[0]\n",
    "                text_data[key] = value\n",
    "    return text_data\n",
    "\n",
    "# Read prompt files\n",
    "prompts_path = f\"projects/{project_name}/prompting\"\n",
    "prompts = read_txt_files(prompts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9a636-9ea2-4b64-a87b-af033c022a64",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Chunk Knowledge Files ‚úÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f108a-5d37-4aaa-9d75-3e4ba3ff3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "knowledge_path = f'projects/{project_name}/knowledge/'\n",
    "\n",
    "# Read json that maps files and chunking strtegy\n",
    "json_path = f'projects/{project_name}/config/chunk-strategy.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    chunk_config = json.load(file)\n",
    "\n",
    "## Apply chunking strategy to every file and store results in a list\n",
    "# Go through every file in knowledge base \n",
    "chunks = {}\n",
    "for file in tqdm(chunk_config):\n",
    "    # Get file path\n",
    "    file_path = os.path.join(knowledge_path, file)\n",
    "    # Get topic\n",
    "    topic = os.path.dirname(file)\n",
    "    # Add the topic as key in chunks dict (if not already in it)\n",
    "    if topic not in chunks: chunks[topic] = []\n",
    "    # Read file content\n",
    "    content = read_file(file_path)\n",
    "    # Divide content into chunks\n",
    "    _ = chunk_text(content, **chunk_config[file])\n",
    "    # Append chunks to chunks dict\n",
    "    chunks[topic] += _\n",
    "\n",
    "# Drop empty chunks\n",
    "chunks = {key: [item for item in value if item] for key, value in chunks.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c261022-d22e-4143-8fca-1c288f19bf83",
   "metadata": {},
   "source": [
    "## Inference Pipeline üó£Ô∏èüí¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e27252-0370-4590-830a-08f34bc8124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(messages, temperature=0, top_p=0.5, top_k=-1, frequency_penalty=0, max_tokens=256):\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a lie\"}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    # Define header\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Define payload\n",
    "    payload = {\n",
    "        \"model\": LLM_NAME,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"top_k\": top_k,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "\n",
    "    # Send the POST request to the API\n",
    "    response = requests.post(LLM_API_URL, headers=headers, json=payload)\n",
    "\n",
    "    # Check for errors\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result['choices'][0]['message']['content'].strip()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f2b0a-6780-4695-a8ba-1dbbc14a7bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test LLM API\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a lie\"}\n",
    "]\n",
    "\n",
    "make_inference(messages, temperature=0, top_p=0.1, top_k=1, frequency_penalty=0, max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce41af-cb31-48e4-9a82-23ef0b86b0ee",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Conversation Simulation ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027dcb00-346a-4253-a2ac-87362c506b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Function to process conversation templates\n",
    "def process_templates(input_list):\n",
    "    import re\n",
    "\n",
    "    last_tag = None # Variable to hold the last tag\n",
    "    output_list = []\n",
    "\n",
    "    for item in input_list:\n",
    "        # Remove tags between /.../\n",
    "        new_item = re.sub(r'/[^/]+/', '\\n', item)\n",
    "\n",
    "        # Extract the last tag\n",
    "        if re.search(r'/([^/]+)/', item):\n",
    "            last_tag = re.search(r'/([^/]+)/', item).group(1)\n",
    "\n",
    "        output_list.append(new_item)\n",
    "\n",
    "    return output_list, last_tag\n",
    "\n",
    "### Function to select random number between star and stop with a step (that supports floats)\n",
    "def random_step_choice(start, end, step):\n",
    "    import random\n",
    "    numbers = [start + i * step for i in range(int((end - start) / step) + 1)]\n",
    "    return random.choice(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dc559-3365-425d-ab04-d69d1c8f12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Generation parameters\n",
    "#iterations = configs[\"generation-parameters\"][\"iterations\"]\n",
    "\n",
    "# Main loop\n",
    "synthetic_data = {\"system-prompt\": [], \"user-prompt\": [], \"inference-params\": [], \"generated-text\": [], \"label\": []}\n",
    "for iteration in tqdm(range(iterations)):\n",
    "            \n",
    "    ### Format system prompt template\n",
    "    ## Define system injections\n",
    "    system_injections = {key: random.choice(value) for key, value in injections[\"character\"].items()}\n",
    "    _ = random.choice(list(chunks.keys())) # Choose a random chunk from a random topic\n",
    "    chunk = random.choice(chunks[_])\n",
    "    system_injections[\"knowledge_base\"] = chunk\n",
    "        \n",
    "    system_prompt = prompts[\"system-prompt\"]\n",
    "    system_prompt = system_prompt.format(**system_injections)\n",
    "    \n",
    "    ### Format user prompt template\n",
    "    ## Define conversationa structure     \n",
    "    templates = prompts[\"templates\"].split(\"-\"*136) # Split txt into templates based on separator: \"-\"*136\n",
    "    template = random.choice(templates) # Randomly select a conversation template\n",
    "    template = template.split(\"\\n\\n\") # Split templates into exchanges\n",
    "    num_exchanges = random.choice(list(range(1, k+1))) # Pick 1, 2 or ... k consecutive exchanges from the conversation template\n",
    "    start_index = random.randint(0, len(template) - num_exchanges)\n",
    "    sub_template = template[start_index:start_index + num_exchanges]\n",
    "    sub_template, last_tag = process_templates(sub_template) # Remove all tags /.../ and save the last one \n",
    "    conversation_injections = get_random_items_from_json(injections[\"conversation\"]) # Randomly define injections\n",
    "    random_injections = get_random_items_from_json(injections[\"random\"])\n",
    "    conversation_structure = \"\" # Format conversation template\n",
    "    for exchange in sub_template:\n",
    "        conversation_structure += exchange.format(**conversation_injections).format(**random_injections) \n",
    "    user_injections = {\"conversation_structure\": conversation_structure}\n",
    "    \n",
    "    user_prompt = prompts[\"user-prompt\"]\n",
    "    user_prompt = user_prompt.format(**user_injections)    \n",
    "    \n",
    "    ### Generate text with LLM    \n",
    "    ## Pick inference params at random\n",
    "    param_ranges = configs[\"generation-parameters\"][\"inference\"]\n",
    "    inference_params = {key: round(random_step_choice(**param_ranges[key]),2) for key, value in param_ranges.items()}\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    generated_text = make_inference(messages, **inference_params)\n",
    "    label = last_tag\n",
    "    \n",
    "    ### Store prompts, text and label in synthetic_data  dict (if not None)\n",
    "    results = {\n",
    "        \"system-prompt\": system_prompt,\n",
    "        \"user-prompt\": user_prompt,\n",
    "        \"inference-params\": inference_params,\n",
    "        \"generated-text\": generated_text,\n",
    "        \"label\": label\n",
    "    }\n",
    "    if not any(element is None for element in results.values()):\n",
    "        for key, value in results.items():\n",
    "            synthetic_data[key] += [value]        \n",
    "\n",
    "print(\"_\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c07a67-8758-4875-8718-1e1a8d2f3fdd",
   "metadata": {},
   "source": [
    "## Handwritten Examples ‚úçÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6482b-c6aa-466b-866a-1c37a58a4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read handwritten examples from txt file\n",
    "file_path = f\"projects/{project_name}/examples/examples.txt\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    examples = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb394518-080b-4608-a024-6e79eaeeba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert examples into a list\n",
    "examples = examples.split(\"\\n\\n\")\n",
    "\n",
    "# Remove empty elements\n",
    "examples = [e for e in examples if e]\n",
    "\n",
    "# Split the strings into 'text' and 'label', and rep each instance n times\n",
    "n = 3\n",
    "cols = [(item.split(' /')[0], item.split('/')[1]) for item in examples for _ in range(n)]\n",
    "\n",
    "# Create the df\n",
    "examples_df = pd.DataFrame(cols, columns=['text', 'label'])\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "examples_df = examples_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Take a look\n",
    "print(examples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac788612-18be-4419-b4de-c7cd19eefbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add values from examples df to synthetic data dict\n",
    "\n",
    "for idx, row in examples_df.iterrows():\n",
    "    results = {\n",
    "        \"system-prompt\": \"<handwritten>\",\n",
    "        \"user-prompt\": \"<handwritten>\",\n",
    "        \"inference-params\": \"<handwritten>\",\n",
    "        \"generated-text\": row.text,\n",
    "        \"label\": row.label\n",
    "    }\n",
    "\n",
    "    for key, value in results.items():\n",
    "        synthetic_data[key] += [value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004dc89-1e26-4c4c-9573-f0fa1b814d1d",
   "metadata": {},
   "source": [
    "## Export Files ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6344a78-93aa-4a74-b68a-6bbed148a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert synthetic_data dict into output dict with label studio valid format\n",
    "\n",
    "output = []\n",
    "for i in range(len(synthetic_data[\"generated-text\"])):\n",
    "    # Read json file with valid output format\n",
    "    file_path = f'projects/{project_name}/config/output-format.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        format_ = json.load(file)\n",
    "\n",
    "    # Append generated data\n",
    "    format_[\"id\"] += i\n",
    "    format_[\"data\"][\"text\"] = synthetic_data[\"generated-text\"][i]\n",
    "    format_[\"annotations\"][0][\"result\"][0][\"value\"][\"choices\"][0] = synthetic_data[\"label\"][i]\n",
    "    output += [format_.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc260f3-87be-41ed-9a22-6cdb39f41b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current time\n",
    "current_datetime = datetime.now().strftime(\"%y-%m-%d-%Hh%Mm%Ss\")\n",
    "\n",
    "# Write generated text and labels (output dict) into .json file\n",
    "file_path = f'projects/{project_name}/generated/{current_datetime}.json'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(output, file)\n",
    "\n",
    "# Write generated text, labels and promots (synthetic_data) to .csv file\n",
    "file_path = f'projects/{project_name}/generated/{current_datetime}.csv'\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67adfa-7c21-44f3-9596-0e5a091ad4f9",
   "metadata": {},
   "source": [
    "## Take a Look üëÅÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156703f-f90e-4a92-9115-83be6ab6d194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    label = df.loc[i][\"label\"]\n",
    "    if label == \"spam\":\n",
    "        bprint(df.loc[i][\"generated-text\"])\n",
    "        bprint(\"-\"*25)\n",
    "        print(df.loc[i][\"user-prompt\"])\n",
    "        bprint(\"_\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c064194-bc28-4f50-b949-fd70fde8d66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate frequency\n",
    "frequency = df[\"label\"].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(5, 4))\n",
    "frequency.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(f'Frequency of Labels in Genedated Data', fontsize=12)\n",
    "plt.xlabel('Label', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe2761-bced-4707-a9bf-6b085a1928e2",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
